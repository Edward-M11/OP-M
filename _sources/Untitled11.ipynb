{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1MqeDpb3IyU"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "import numpy as np\n",
        "\n",
        "# Cargamos el conjunto de datos Iris\n",
        "iris = datasets.load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "\n",
        "# Seleccionamos solo las clases Iris Setosa (0) e Iris Virginica (2) y reemplazamos las clases por -1 y 1 para ajustarse al problema SVM\n",
        "indices = np.where((y == 0) | (y == 2))\n",
        "X = X[indices]\n",
        "y = y[indices]\n",
        "y = np.where(y == 0, -1, 1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos cargado con éxito el conjunto de datos de Iris y seleccionado solo las clases Iris Setosa e Iris Virginica. Las características (X) tienen dimensiones (100, 4), lo que significa que hay 100 muestras con 4 características cada una. Las etiquetas (y) son un vector de 100 elementos, que ahora contienen -1 para Iris Setosa y 1 para Iris Virginica."
      ],
      "metadata": {
        "id": "Ljpqess13J3J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente paso es construir el problema dual de SVM. Para ello, definiremos la función objetivo y las restricciones del problema. Luego, utilizaremos la función de kernel radial y resolveremos el problema de optimización con el método de puntos interiores utilizando la biblioteca cvxopt."
      ],
      "metadata": {
        "id": "sf8y4K6a88IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Parámetros del Problema de Optimización Cuadrática**\n",
        "El problema de optimización para el SVM dual:\n",
        " $\\text{Maximizar:}\n",
        "\\quad ∑_{i=1}^{n}\\alpha_i-\\frac{1}{2}∑_{i,j=1}^{n} y_iy_jα_iα_jx_i⋅x_j\n",
        "\\\\\n",
        "\\text{Sujeto a:}\n",
        "\\quad 0 \\leq α_i \\leq C,\n",
        "\\quad ∑_{i,j=1}^{n}α_iy_i =0\n",
        "$\n",
        "\n",
        " se puede escribir en forma matricial como:\n",
        "  $\\text{Minimizar:}\n",
        "\\quad \\frac{1}{2} \\alpha^T P \\alpha - q^T \\alpha\n",
        "\\\\\n",
        "\\text{Sujeto a:}\n",
        "\\quad G \\alpha \\leq h ,\n",
        "\\quad A \\alpha = b\n",
        "$\n",
        "\n",
        "Donde:\n",
        "\n",
        "\n",
        "*   α son los multiplicadores de Lagrange.\n",
        "*   P es una matriz que en el caso de SVM es el producto del kernel de todas las muestras y el producto externo de las etiquetas de las clases, es decir, $P = y_iy_jK_{ij}$\n",
        "*  q es un vector columna con todos los elementos igual a -1, ya que en el problema dual, buscamos maximizar la suma de los $α_i$ (minimizar el negativo de la suma)\n",
        "*   G y h son matrices que representan las restricciones de desigualdad de los $α_i$, que deben estar entre 0 y el valor de penalizacion C. Aquí, G es una matriz que se construye para mantener $α\\geq 0$ y $α\\leq C$    \n",
        "*   A y b representan la restricción de igualdad que asegura que la suma de los productos de los multiplicadores de Lagrange y las etiquetas de las clases es 0, es decir, $∑y_iα_i=0$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RjZ2v55--64Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from cvxopt import matrix, solvers\n",
        "\n",
        "# Definimos la función del kernel radial (RBF)\n",
        "def kernel_rbf(x, y, gamma=0.5):\n",
        "    return np.exp(-gamma * np.linalg.norm(x - y) ** 2)\n",
        "\n",
        "# Construimos la matriz del kernel para todas las muestras\n",
        "K = np.zeros((len(X), len(X)))\n",
        "for i in range(len(X)):\n",
        "    for j in range(len(X)):\n",
        "        K[i,j] = kernel_rbf(X[i], X[j])\n",
        "\n",
        "# Configuramos los parámetros para el problema de optimización cuadrática\n",
        "P = matrix(np.outer(y, y) * K)\n",
        "q = matrix(-np.ones((len(X), 1)))\n",
        "G = matrix(np.vstack((-np.eye(len(X)), np.eye(len(X)))))\n",
        "h = matrix(np.hstack((np.zeros(len(X)), np.ones(len(X)) * 1))) # C=1\n",
        "A = matrix(y.reshape(1, -1).astype(np.double))\n",
        "b = matrix(np.zeros(1), (1, 1))\n",
        "\n",
        "# Resolvemos el problema de optimización cuadrática\n",
        "solvers.options['show_progress'] = True\n",
        "solvers.options['abstol'] = 1e-10\n",
        "solvers.options['reltol'] = 1e-10\n",
        "solvers.options['feastol'] = 1e-10\n",
        "\n",
        "# Medimos el tiempo antes de comenzar la optimización\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "solution = solvers.qp(P, q, G, h, A, b)\n",
        "\n",
        "# Tiempo total de ejecución y número de iteraciones\n",
        "execution_time = time.time() - start_time\n",
        "num_iterations = solution['iterations']\n",
        "\n",
        "# Mostramos el tiempo de ejecución y el número de iteraciones\n",
        "execution_time, num_iterations\n",
        "\n",
        "# Extrae los multiplicadores de Lagrange\n",
        "lambdas = np.array(solution['x']).flatten()\n",
        "\n",
        "# Identifica los vectores de soporte (aquellos con lambda no insignificante)\n",
        "support_vectors_idx = lambdas > 1e-5\n",
        "support_vectors = X[support_vectors_idx]\n",
        "support_vector_labels = y[support_vectors_idx]\n",
        "support_vector_lambdas = lambdas[support_vectors_idx]\n",
        "\n",
        "# Muestra la información relevante\n",
        "print(\"Número de vectores de soporte:\", support_vectors.shape[0])\n",
        "print(\"Vectores de soporte:\")\n",
        "print(support_vectors)\n"
      ],
      "metadata": {
        "id": "nFnrKokD3W9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51f70ec0-9b28-4266-ae81-f5cefc563e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     pcost       dcost       gap    pres   dres\n",
            " 0: -1.2253e+00 -1.1560e+02  4e+02  1e+00  6e-16\n",
            " 1:  7.6781e-01 -4.0758e+01  4e+01  3e-16  7e-16\n",
            " 2: -1.8193e+00 -5.5004e+00  4e+00  6e-16  7e-16\n",
            " 3: -2.2581e+00 -3.1365e+00  9e-01  4e-16  4e-16\n",
            " 4: -2.3838e+00 -2.6883e+00  3e-01  2e-16  5e-16\n",
            " 5: -2.4701e+00 -2.5619e+00  9e-02  2e-16  5e-16\n",
            " 6: -2.4967e+00 -2.5011e+00  4e-03  2e-16  5e-16\n",
            " 7: -2.4986e+00 -2.4987e+00  1e-04  2e-16  5e-16\n",
            " 8: -2.4986e+00 -2.4986e+00  5e-06  5e-16  5e-16\n",
            " 9: -2.4986e+00 -2.4986e+00  1e-07  5e-16  6e-16\n",
            "10: -2.4986e+00 -2.4986e+00  1e-09  5e-16  5e-16\n",
            "11: -2.4986e+00 -2.4986e+00  1e-11  8e-16  6e-16\n",
            "Optimal solution found.\n",
            "Número de vectores de soporte: 14\n",
            "Vectores de soporte:\n",
            "[[4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [6.3 3.3 6.  2.5]\n",
            " [4.9 2.5 4.5 1.7]\n",
            " [7.7 3.8 6.7 2.2]\n",
            " [7.7 2.6 6.9 2.3]\n",
            " [6.  2.2 5.  1.5]\n",
            " [7.9 3.8 6.4 2. ]\n",
            " [6.1 2.6 5.6 1.4]\n",
            " [6.9 3.1 5.1 2.3]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**pcost y dcost**: Estos son el coste primal y dual, respectivamente, en cada iteración del algoritmo. El coste primal se refiere al valor de la función objetivo en el espacio original de variables, mientras que el coste dual es el valor en el espacio de las variables duales. Al principio, estos valores son muy diferentes, lo que indica que la solución aún no es óptima.\n",
        "\n",
        "**gap**: Esta es la diferencia entre el coste primal y el coste dual. A medida que el algoritmo progresa, este \"gap\" debe disminuir, indicando que la solución primal y dual se están acercando a la convergencia. Cuando el gap es pequeño, estamos cerca de la solución óptima.\n",
        "\n",
        "**pres**: La \"presión\" primal o presión residual primal. Es una medida de cuán bien se están cumpliendo las restricciones del problema primal. Una presión pequeña significa que las restricciones se están satisfaciendo bien.\n",
        "\n",
        "**dres**: La presión residual dual. Similar a la presión primal, pero para el problema dual. También es una medida de la satisfacción de las restricciones, pero en el espacio dual."
      ],
      "metadata": {
        "id": "b_aplv1_-0ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Algoritmo del Método de Puntos Interiores\n",
        "\n",
        "### Paso 1: Inicialización\n",
        "\n",
        "Se selecciona un punto inicial  $x_0$ en el interior del conjunto factible y se establece un parámetro de barrera inicial  μ .\n",
        "\n",
        "### Paso 2: Solución de Subproblemas\n",
        "\n",
        "En cada iteración \\( k \\), se resuelve el siguiente subproblema cuadrático:\n",
        "\n",
        "\\[\n",
        "\\begin{aligned}\n",
        "\\text{minimizar} \\quad & f_0(\\mathbf{x}) - \\mu_k \\sum_{i=1}^{m} \\log(-f_i(\\mathbf{x})),\n",
        "\\end{aligned}\n",
        "\\]\n",
        "\n",
        "donde $μ_k$ es el valor actual del parámetro de barrera. Este subproblema se resuelve típicamente mediante métodos de Newton.\n",
        "\n",
        "### Paso 3: Actualización del Parámetro de Barrera\n",
        "\n",
        "El parámetro de barrera μ se reduce según una estrategia predefinida, como $ \\mu_{k+1} = \\rho \\mu_k $ con $ 0 < \\rho < 1 $.\n",
        "\n",
        "### Paso 4: Condiciones de Terminación\n",
        "\n",
        "El algoritmo verifica la convergencia basándose en criterios como la norma del gradiente de la función objetivo modificada o la proximidad a las condiciones de Karush-Kuhn-Tucker (KKT).\n",
        "\n",
        "### Paso 5: Extracción de la Solución\n",
        "\n",
        "Al converger, se extrae la solución actual como la aproximación óptima."
      ],
      "metadata": {
        "id": "ZKDnZeG-GX30"
      }
    }
  ]
}